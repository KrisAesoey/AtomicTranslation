{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import torchtext\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from TorchTransformer import *\n",
    "from evaluation import *"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# GLOBAL VARIABLES\n",
    "PAD_IDX = 0\n",
    "SOS_TOKEN = 1\n",
    "EOS_TOKEN = 2\n",
    "MAX_LENGTH = 50\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Select which dataset to run experiments on by specifying the length."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "DATASET = \"20k\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def yield_tokens(lines):\n",
    "    for line in lines:\n",
    "        yield line.split()\n",
    "\n",
    "def create_vocab(lang):\n",
    "    vocab = build_vocab_from_iterator(yield_tokens(lang), specials=[\"<PAD>\", \"<SOS>\", \"<EOS>\"], special_first=True)\n",
    "    vocab.set_default_index(-1)\n",
    "    return vocab\n",
    "\n",
    "def read_data(dataset=\"all\"):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = []\n",
    "    with open('data_datasets/' + dataset + '.csv') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            lines.append(row)\n",
    "            \n",
    "    print(lines[0])\n",
    "    \n",
    "    lang1 = list(map(lambda x: x[0], lines))\n",
    "    lang2 = list(map(lambda x: x[1], lines))\n",
    "\n",
    "    return lang1, lang2, lines\n",
    "\n",
    "def add_sequence_tokens(sentence):\n",
    "    new_sentence = sentence.split()\n",
    "    n_tokens = len(new_sentence)\n",
    "    new_sentence.insert(0, \"<SOS>\")\n",
    "    new_sentence.append(\"<EOS>\")\n",
    "    assert n_tokens + 2 == len(new_sentence)\n",
    "    return \" \".join(new_sentence)\n",
    "\n",
    "def add_sequence_tokens_dataset(data):\n",
    "    new_data = []\n",
    "    for d in data:\n",
    "        new_data.append(add_sequence_tokens(d))\n",
    "    return new_data\n",
    "\n",
    "def tensor_from_sentence(sentence, vocab):\n",
    "    encoded = [vocab[word] for word in sentence.split()]\n",
    "    return torch.tensor(encoded, dtype=torch.long, device=device)\n",
    "\n",
    "def filter_pairs(pairs, MAX_LENGTH):\n",
    "    filtered_pairs = []\n",
    "    for s1, s2 in pairs:\n",
    "        if len(s1.split()) < MAX_LENGTH and len(s2.split()) < MAX_LENGTH:\n",
    "            filtered_pairs.append([s1, s2])\n",
    "            \n",
    "    lang1 = list(map(lambda x: x[0], filtered_pairs))\n",
    "    lang2 = list(map(lambda x: x[1], filtered_pairs))\n",
    "    print(filtered_pairs[0])\n",
    "    return lang1, lang2, filtered_pairs\n",
    "\n",
    "def pad_dataset(data, target_length):\n",
    "    new_data = [] \n",
    "    for d in data:\n",
    "        x = d.split()\n",
    "        x = x + [\"<PAD>\"] * (target_length - len(x))\n",
    "        new_data.append(\" \".join(x))\n",
    "    return new_data\n",
    "\n",
    "def create_dataset(src_lang, src_vocab, trg_lang, trg_vocab, MAX_LENGTH):\n",
    "    src_list = []\n",
    "    trg_list = []\n",
    "    \n",
    "    src_lang = pad_dataset(src_lang, MAX_LENGTH)\n",
    "    trg_lang = pad_dataset(trg_lang, MAX_LENGTH)\n",
    "    \n",
    "    for src, trg in zip(src_lang, trg_lang):\n",
    "        src_list.append(tensor_from_sentence(src, src_vocab).to(device))\n",
    "        trg_list.append(tensor_from_sentence(trg, trg_vocab).to(device))\n",
    "    src_tensors = torch.stack(src_list)\n",
    "    trg_tensors = torch.stack(trg_list)\n",
    "    return TensorDataset(src_tensors, trg_tensors)\n",
    "\n",
    "def create_dataset_new(src_lang, src_vocab, trg_lang, trg_vocab, MAX_LENGTH):\n",
    "    src_list = []\n",
    "    trg_list = []\n",
    "    \n",
    "    src_lang = add_sequence_tokens_dataset(src_lang)\n",
    "    trg_lang = add_sequence_tokens_dataset(trg_lang)\n",
    "    \n",
    "    src_lang = pad_dataset(src_lang, MAX_LENGTH + 2)\n",
    "    trg_lang = pad_dataset(trg_lang, MAX_LENGTH + 2)\n",
    "    \n",
    "    for src, trg in zip(src_lang, trg_lang):\n",
    "        src_list.append(tensor_from_sentence(src, src_vocab).to(device))\n",
    "        trg_list.append(tensor_from_sentence(trg, trg_vocab).to(device))\n",
    "    src_tensors = torch.stack(src_list)\n",
    "    trg_tensors = torch.stack(trg_list)\n",
    "    return TensorDataset(src_tensors, trg_tensors)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "_, _, train_pairs = read_data(\"dket_train_\" + DATASET)\n",
    "_, _, val_pairs = read_data(\"dket_validation_\" + DATASET)\n",
    "\n",
    "train_text, train_logic, _ = filter_pairs(train_pairs, MAX_LENGTH)\n",
    "val_text, val_logic, _ = filter_pairs(val_pairs, MAX_LENGTH)\n",
    "text_vocab = create_vocab(train_text + val_text)\n",
    "logic_vocab = create_vocab(train_logic + val_logic)\n",
    "\n",
    "print(len(train_text), len(train_logic))\n",
    "print(len(val_text), len(val_logic))\n",
    "print(len(text_vocab), print(len(logic_vocab)))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reading lines...\n",
      "['kernel summary of trunk forgive also principal of string or of fever .', 'kernel summary of trunk := E forgive . ( principal of string U principal of fever )']\n",
      "Reading lines...\n",
      "['every rural guilty kernel forgive or plug no summary of stateful trunk or of principal .', 'rural guilty kernel := ! E ( forgive ^ plug ) . ( summary of stateful trunk U summary of principal )']\n",
      "Reading lines...\n",
      "[\"personx returns to personx's work xintent to keep their job\", 'person (x) & returns to (x,a) & work (a) -> to keep their job (x)']\n",
      "[\"personx returns to personx's work xintent to keep their job\", 'person (x) & returns to (x,a) & work (a) -> to keep their job (x)']\n",
      "['kernel summary of trunk forgive also principal of string or of fever .', 'kernel summary of trunk := E forgive . ( principal of string U principal of fever )']\n",
      "['every rural guilty kernel forgive or plug no summary of stateful trunk or of principal .', 'rural guilty kernel := ! E ( forgive ^ plug ) . ( summary of stateful trunk U summary of principal )']\n",
      "20000 20000\n",
      "30000 30000\n",
      "24680\n",
      "24679 None\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "train_dataset = create_dataset_new(train_text, text_vocab, train_logic, logic_vocab, MAX_LENGTH)\n",
    "val_dataset = create_dataset_new(val_text, text_vocab, val_logic, logic_vocab, MAX_LENGTH)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "src_vocab_size = len(text_vocab)\n",
    "trg_vocab_size = len(logic_vocab)\n",
    "embed_dim=512\n",
    "transformer = Transformer(src_vocab_size,\n",
    "                          trg_vocab_size,\n",
    "                          embed_size=embed_dim,\n",
    "                          max_length=MAX_LENGTH+2,\n",
    "                          dropout=0.1,\n",
    "                          pad_idx=PAD_IDX).to(device)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(transformer):,} trainable parameters')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The model has 82,126,440 trainable parameters\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "class Scheduler(_LRScheduler):\n",
    "    def __init__(self, \n",
    "                 optimizer: Optimizer,\n",
    "                 dim_embed: int,\n",
    "                 warmup_steps: int,\n",
    "                 last_epoch: int=-1,\n",
    "                 verbose: bool=False) -> None:\n",
    "\n",
    "        self.dim_embed = dim_embed\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.num_param_groups = len(optimizer.param_groups)\n",
    "\n",
    "        super().__init__(optimizer, last_epoch, verbose)\n",
    "        \n",
    "    def get_lr(self) -> float:\n",
    "        lr = calc_lr(self._step_count, self.dim_embed, self.warmup_steps)\n",
    "        return [lr] * self.num_param_groups\n",
    "\n",
    "\n",
    "def calc_lr(step, dim_embed, warmup_steps):\n",
    "    return dim_embed**(-0.5) * min(step**(-0.5), step * warmup_steps**(-1.5))\n",
    "\n",
    "class TranslationLoss(nn.Module):\n",
    "    def __init__(self, label_smoothing: float=0.0) -> None:\n",
    "        super().__init__()\n",
    "        self.loss_func = nn.CrossEntropyLoss(ignore_index    = PAD_IDX,\n",
    "                                             label_smoothing = label_smoothing)\n",
    "\n",
    "    def forward(self, logits: Tensor, labels: Tensor) -> Tensor:\n",
    "        vocab_size = logits.shape[-1]\n",
    "        logits = logits.reshape(-1, vocab_size)\n",
    "        labels = labels.reshape(-1).long()\n",
    "        return self.loss_func(logits, labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def train(model: nn.Module,\n",
    "          loader: DataLoader,\n",
    "          loss_func: torch.nn.Module,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          scheduler: torch.optim.lr_scheduler._LRScheduler) -> float:\n",
    "\n",
    "    model.train() # train mode\n",
    "    \n",
    "    total_loss = 0\n",
    "    num_batches = len(loader)\n",
    "\n",
    "    for source, target in tqdm(loader):\n",
    "        # feed forward\n",
    "        logits = model(source[:, 1:], target[:, :-1]) #input lacking EOS\n",
    "\n",
    "        # loss calculation\n",
    "        loss = loss_func(logits, target[:, 1:]) #labels lacking SOS\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # back-prop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # learning rate scheduler\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "    # average training loss\n",
    "    avg_loss = total_loss / num_batches\n",
    "    return avg_loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "optimizer = optim.Adam(transformer.parameters(),\n",
    "                   betas=(0.9, 0.98),\n",
    "                   eps=1.0e-9)\n",
    "criterion = TranslationLoss(label_smoothing=0.1)\n",
    "scheduler = Scheduler(optimizer=optimizer, dim_embed=embed_dim, warmup_steps=4000)\n",
    "print(\"Training\", DATASET, \":\")\n",
    "best_res = float(\"inf\")\n",
    "for e in range(30):\n",
    "    res = train(transformer, train_loader, criterion, optimizer, scheduler)\n",
    "    print(res)\n",
    "    if res < best_res:\n",
    "        best_res = res\n",
    "    else:\n",
    "        break\n",
    "print(\"------Training complete-----\")\n",
    "\n",
    "torch.save(transformer.state_dict(), \"./models/\" + \"dket_\" + DATASET + \".pt\")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training 20k :\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|█████████████████████████████████████████| 313/313 [01:03<00:00,  4.94it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "6.815534914644381\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|█████████████████████████████████████████| 313/313 [01:03<00:00,  4.95it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4.636190796050782\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|█████████████████████████████████████████| 313/313 [01:03<00:00,  4.94it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3.991013415705282\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|█████████████████████████████████████████| 313/313 [01:03<00:00,  4.93it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3.5188538670158995\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|█████████████████████████████████████████| 313/313 [01:03<00:00,  4.94it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.8477525474926155\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|█████████████████████████████████████████| 313/313 [01:03<00:00,  4.93it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.130113551030144\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|█████████████████████████████████████████| 313/313 [01:03<00:00,  4.94it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.6220525190853083\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|█████████████████████████████████████████| 313/313 [01:03<00:00,  4.94it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.4759323494122052\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|█████████████████████████████████████████| 313/313 [01:03<00:00,  4.94it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.436278932391645\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|█████████████████████████████████████████| 313/313 [01:03<00:00,  4.95it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.4175962857164133\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|█████████████████████████████████████████| 313/313 [01:03<00:00,  4.95it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.4064271370062051\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|█████████████████████████████████████████| 313/313 [01:03<00:00,  4.95it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.3995328242786396\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|█████████████████████████████████████████| 313/313 [01:03<00:00,  4.94it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.39464743335407\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|█████████████████████████████████████████| 313/313 [01:03<00:00,  4.94it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.3895857197027237\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|█████████████████████████████████████████| 313/313 [01:03<00:00,  4.95it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.3839232750213184\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|█████████████████████████████████████████| 313/313 [01:02<00:00,  5.03it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.3797697057358373\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|█████████████████████████████████████████| 313/313 [01:01<00:00,  5.07it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.376134115667008\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|█████████████████████████████████████████| 313/313 [01:01<00:00,  5.05it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.3734247585455068\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|█████████████████████████████████████████| 313/313 [01:01<00:00,  5.07it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.3710491600128027\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|█████████████████████████████████████████| 313/313 [01:01<00:00,  5.09it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.3688881701935594\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|█████████████████████████████████████████| 313/313 [01:01<00:00,  5.08it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.3676086313808307\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|█████████████████████████████████████████| 313/313 [01:01<00:00,  5.07it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.3662923730600376\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|█████████████████████████████████████████| 313/313 [01:01<00:00,  5.07it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.3647111174397575\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|█████████████████████████████████████████| 313/313 [01:01<00:00,  5.07it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.3636334132843506\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|█████████████████████████████████████████| 313/313 [01:01<00:00,  5.09it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.3629326131016302\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|█████████████████████████████████████████| 313/313 [01:01<00:00,  5.09it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.3619346409179152\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|█████████████████████████████████████████| 313/313 [01:01<00:00,  5.06it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.3610689354399903\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|█████████████████████████████████████████| 313/313 [01:01<00:00,  5.09it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.3601902612862877\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|█████████████████████████████████████████| 313/313 [01:01<00:00,  5.09it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.3598189258727784\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|█████████████████████████████████████████| 313/313 [01:01<00:00,  5.08it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.3590542218936519\n",
      "------Training complete-----\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def evaluation(model, val_dataset, src_vocab, trg_vocab):\n",
    "    src_itos = src_vocab.get_itos()\n",
    "    trg_itos = trg_vocab.get_itos()\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    reference_tokens = []\n",
    "    predicted_tokens = []\n",
    "    for source, target in tqdm(val_dataset):\n",
    "        logits = model(source[1:].unsqueeze(0), target[:-1].unsqueeze(0)) #input lacking EOS\n",
    "        golden = [trg_itos[t] for t in target if t != PAD_IDX][1:]\n",
    "        reference_tokens.append(golden)\n",
    "        target_tokens = []\n",
    "        for word in logits.tolist()[-1]:\n",
    "            guess = trg_itos[np.argmax(word)]\n",
    "            target_tokens.append(guess)\n",
    "            if guess == \"<EOS>\":\n",
    "                break\n",
    "        predicted_tokens.append(target_tokens)\n",
    "        \"\"\"\n",
    "        if target_tokens == golden:\n",
    "            print([src_itos[s] for s in source if s != PAD_IDX][1:])\n",
    "            print(target_tokens, golden)\n",
    "        \"\"\"\n",
    "    \n",
    "    formula = average_formula_accuracy(predicted_tokens, reference_tokens, write_results=False)\n",
    "    token = average_token_accuracy(predicted_tokens, reference_tokens)\n",
    "    edit_distance = average_ld(predicted_tokens, reference_tokens)\n",
    "\n",
    "    return formula, token, edit_distance\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "print(\"Evaluating model:\")\n",
    "formula, token, edit_distance = evaluation(transformer, val_dataset, text_vocab, logic_vocab)\n",
    "print(\"average formula accuracy: \", formula)\n",
    "print(\"average token accuracy: \", token)\n",
    "print(\"average edit distance: \", edit_distance)\n",
    "print(\"-----------------------\")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Evaluating model:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|█████████████████████████████████████| 60000/60000 [49:27<00:00, 20.22it/s]\n",
      "60000it [00:15, 3813.93it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "average formula accuracy:  0.9488333333333333\n",
      "average token accuracy:  0.9963426528984816\n",
      "average edit distance:  0.054966666666666664\n",
      "-----------------------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}